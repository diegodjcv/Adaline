{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Investigación - Practica 3 </center>\n",
    "\n",
    "### Inteligencia Artificial II\n",
    "\n",
    "**Nombre:** Diego Cabrera\n",
    "<hr>\n",
    "\n",
    "### **Red Adeline**\n",
    "\n",
    "\n",
    "\n",
    "* *Describir la arquitectura de la red y su regla de aprendizaje.*\n",
    "\n",
    "<DIV ALIGN=\"justify\">La arquitectura de Adaline (Adaptive Linear Neuron) fue creada por Bernard Widrow en 1959. Utiliza un dispositivo lógico que realiza una suma lineal de las entradas y genera una función umbral para el resultado de dicha suma. La topología de la red ADALINE es similar a la del perceptrón sólo que en este caso la función de salida de las neuronas es lineal. Dado que las señales de entrada pueden ser continuas, la red ADALINE es un dispositivo de entrada/salida analógica (continua) a diferencia del perceptrón que de acuerdo a lo dicho anteriormente es un dispositivo entrada/salida digital (binaria). La operación de una red ADALINE con n neuronas de entrada y m neuronas de salidas puede ser resumida de la siguiente manera: </DIV>\n",
    "\n",
    "<img src=\"formula1.png\">\n",
    "\n",
    "<DIV ALIGN=\"justify\">Sin embargo, la principal diferencia entre la red ADALINE y el perceptrón consiste en la regla de aprendizaje que utilizan. En el caso de la red ADALINE implementa como método de aprendizaje la regla de Widrow-Hoff, también conocida como regla LMS (Least Mean Squares, mínimos cuadrados), que realiza una actualización continua de los pesos sinápticos de acuerdo a la contribución de cada neurona sobre el error total de la red.</DIV>\n",
    "\n",
    "* **Estructura Adaline**\n",
    "\n",
    "<DIV ALIGN=\"justify\"> La unidad procesadora representada por un círculo con el símbolo sumatorio implementa una función umbral. Las conexiones de cada una de las entradas tienen asociadas un valor de ponderación llamado también peso wi. \n",
    "    </DIV>\n",
    "\n",
    "<DIV ALIG=\"justify\">El Adaline es Lineal porque la salida es una función lineal sencilla de los valores de la entrada. Es una Neurona tan solo en el sentido (muy limitado) del PE. También se podría decir que el Adaline es un Elemento Lineal, evitando por completo la definición como Neurona.Si se combinan varios adalines se obtiene la configuración denominada Madaline</DIV>\n",
    "\n",
    "<DIV ALIGN=\"justify\">La estructura general de la red tipo Adaline puede visualizarse en la siguiente figura: </DIV>\n",
    "\n",
    "<img src=\"formula2.png\">\n",
    "\n",
    "\n",
    "\n",
    "* Perceptron Multicapa\n",
    "\n",
    "<DIV ALIGN=\"justify\">El perceptrón multicapa es una extensión del perceptrón simple. La topología de un perceptrón multicapa esta definida por un conjunto de capas ocultas, una capa de entrada y una de salida. No existen restricciones sobre la función de activación aunque en general se suelen utilizar funciones sigmoideas. </DIV>\n",
    "\n",
    "\n",
    "* Algoritmo de aprendizaje\n",
    "<DIV ALIGN=\"justify\"> Primero se realiza el cálculo de la suma ponderada de las N entradas:</DIV>\n",
    "\n",
    "<img src=\"formula2.png\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<DIV ALIGN=\"justify\"> El orden de operaciones de ADALINE es el siguiente:\n",
    "\n",
    "1. Presentar un patrón de entrada 𝑋 = (𝑥1, 𝑥2, 𝑥3 … 𝑥𝑁)\n",
    "2. Calcular la salida y = w1x1 + w2x2 + w3x3 … w𝑁x𝑁\n",
    "3. Comparar con la salida esperada E= (𝑑 − y)\n",
    "4. Calcular la actualización pesos y el umbral.\n",
    "∆𝑝𝑤𝑗 = 𝛾 · ∆𝑝 · 𝑥𝑗\n",
    "5. Modificar los pesos y el umbral\n",
    "𝑤𝑗(𝑡 + 1) = 𝑤𝑗(𝑡) + ∆𝑝𝑤𝑗\n",
    " 𝜃(𝑡 + 1) = 𝜃(𝑡) + ∆𝑝𝜃\n",
    "6. Repetir de 1 a 5 hasta acabar con todos los patrones de entrada\n",
    "= Completar un ciclo de aprendizaje\n",
    "7. Realizar ciclos de aprendizaje (Repetir de 1 a 6) hasta alcanzar el\n",
    "criterio de parada. </DIV>\n",
    "    \n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "<img src=\"ejercicio.png\" > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=  [[0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "W=  [0.78 0.66]\n",
      "ALFA=  0.8\n",
      "NUM: 0 NUEVOS PESOS: 2.01   1.18\n",
      "NUM: 1 NUEVOS PESOS: 1.97   1.01\n",
      "NUM: 2 NUEVOS PESOS: 2.0   1.0\n",
      "NUM: 3 NUEVOS PESOS: 2.0   1.0\n",
      "NUM: 4 NUEVOS PESOS: 2.0   1.0\n",
      "NUM: 5 NUEVOS PESOS: 2.0   1.0\n",
      "NUM: 6 NUEVOS PESOS: 2.0   1.0\n",
      "NUM: 7 NUEVOS PESOS: 2.0   1.0\n",
      "NUM: 8 NUEVOS PESOS: 2.0   1.0\n",
      "NUM: 9 NUEVOS PESOS: 2.0   1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "alfa = 0.8\n",
    "    \n",
    "INPUTS = np.array([[0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]\n",
    "                   ])\n",
    "        \n",
    "OUTPUTS = np.array([[1,2,3]]).T\n",
    "\n",
    "WEIGHTS = np.array([0.78,0.66])\n",
    "\n",
    "print(\"X= \",INPUTS)\n",
    "print(\"W= \",WEIGHTS)\n",
    "print(\"ALFA= \",alfa)\n",
    "\n",
    "\n",
    "errors=[]\n",
    "w= []\n",
    "# Training loop\n",
    "\n",
    "for i in range(10):\n",
    "    for fila in range(3):\n",
    "        #print(\"PESOS:\",WEIGHTS[0],\" \",WEIGHTS[1],\" \",WEIGHTS[2])\n",
    "        y=(INPUTS[fila][0]*WEIGHTS[0])+(INPUTS[fila][1]*WEIGHTS[1])\n",
    "        #print(\"VALOR:\",INPUTS[fila][0],\"VALOR:\",INPUTS[fila][1])\n",
    "        #print(\"y=\",y)\n",
    "\n",
    "        error = OUTPUTS[fila]-y\n",
    "        #print(\"ERROR\",error)\n",
    "\n",
    "        WEIGHTS[0]=WEIGHTS[0]+(alfa*error*INPUTS[fila][0])\n",
    "        WEIGHTS[1]=WEIGHTS[1]+(alfa*error*INPUTS[fila][1])\n",
    "        #print(\"NUEVOS PESOS:\",WEIGHTS[0],\" \",WEIGHTS[1])\n",
    "        #print(\"\\n\")\n",
    "    print(\"NUM:\",i,\"NUEVOS PESOS:\",round(WEIGHTS[0],2),\" \",round(WEIGHTS[1],2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Referencias**\n",
    "<br>\n",
    "\n",
    "**[1]** Britos, M. I. P. (2005). *Entrenamiento de redes neuronales basado en algoritmos evolutivos* (Doctoral dissertation, UNIVERSIDAD DE BUENOS AIRES).\n",
    "\n",
    "**[2]** Olabe, X. B. (1998). *Redes neuronales artificiales y sus aplicaciones. Publicaciones de la Escuela de Ingenieros.*\n",
    "\n",
    "**[3]** Tanco, F. (2003). *Introducción a las redes neuronales artificiales. Grupo de Inteligencia Artificial.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7-simulacion",
   "language": "python",
   "name": "simulacion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
